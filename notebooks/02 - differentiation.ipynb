{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    u_{i,j} =\n",
    "    \\begin{bmatrix}\n",
    "      \\frac{\\partial u_1}{\\partial x_1} & \n",
    "        \\frac{\\partial u_1}{\\partial x_2} & \n",
    "        \\frac{\\partial u_1}{\\partial x_3} \\\\[1ex] % <-- 1ex more space between rows of matrix\n",
    "      \\frac{\\partial u_2}{\\partial x_1} & \n",
    "        \\frac{\\partial u_2}{\\partial x_2} & \n",
    "        \\frac{\\partial u_2}{\\partial x_3} \\\\[1ex]\n",
    "      \\frac{\\partial u_3}{\\partial x_1} & \n",
    "        \\frac{\\partial u_3}{\\partial x_2} & \n",
    "        \\frac{\\partial u_3}{\\partial x_3}\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `autograd` package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This package provides automatic differentiation for Tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All `torch.Tensor` objects have an attribute called `.requires_grad`. When we set it as `True`, torch starts to track all the operations in order to compute its gradient. When all the computation is done, we can call the `.backward()` method, and the gradients will be computed and accumulated into `.grad` attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want a `torch.Tensor` to stop tracking operations, we should call `.detach()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `torch.Tensor` that uses `.requires_grad`, uses its `.grad_fn` to store an auxiliar function that helps `autograd` to compute gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simple example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $f$ be a differentiable function as, for example, $f = \\frac{1}{10} (x - 2)^2$. This way, $\\frac{\\partial f}{\\partial x} = 5 (x - 2)$.\n",
    "\n",
    "Now, lets suppose that $x = 1$. The gradient at this point shuold be $\\frac{\\partial f}{\\partial x} = -5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1000], grad_fn=<MulBackward0>) <MulBackward0 object at 0x125c76b70>\n"
     ]
    }
   ],
   "source": [
    "y = 1/10 * (x) **2\n",
    "print(y, y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradiend should be 1/5, and the result is: tensor([0.2000])\n"
     ]
    }
   ],
   "source": [
    "print(\"The gradiend should be 1/5, and the result is:\", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
